# Introdução ao PySpark com RDDs
Este projeto demonstra um exemplo básico de uso do Apache Spark com PySpark para a criação e manipulação de RDDs (Resilient Distributed Datasets). O código inclui desde a configuração de uma SparkSession até a aplicação de transformações e ações em RDDs.

## Visão Geral
O PySpark é a interface Python para o Apache Spark, que oferece um framework poderoso para processamento distribuído de grandes volumes de dados. Neste projeto, você aprenderá:

<ul>
  <li>Como configurar um ambiente Spark usando PySpark.</li>
  <li>Como criar e manipular RDDs.</li>
  <li>Como aplicar transformações e ações em RDDs.</li>
  <li>Boas práticas para o uso de exceções e finalização de uma SparkSession.</li>
</ul>

## Funcionalidades
<ul>
  <li>Instalação do PySpark: Garantia de que o PySpark está configurado corretamente.</li>
  <li>Configuração do SparkSession: Exemplo de como iniciar uma sessão Spark com parâmetros customizados.</li>
  <li>RDDs: Criação e manipulação de RDDs com transformações e ações básicas.</li>
  <li>Tratamento de Exceções: Uso de blocos try-except para capturar e tratar erros.</li>
  <li>Encerramento do Spark: Garantia de que a sessão Spark será finalizada corretamente.</li>
</ul>
